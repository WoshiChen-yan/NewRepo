graph TD
    subgraph "外部环境与初始化"
        A["系统启动与初始化"] --> B{"构建容器化仿真平台"};
        B --> C{"初始化网络拓扑与节点"};
        C --> D{"创建 PPO 智能体"};
        D --> D1("独立 Actor");
        D --> D2("共享 Critic");
        D1 --> D3("带 GRU 的策略网络");
        D2 --> D4("带 GRU 的价值网络");
        D --> E["初始化奖励历史记录"];
    end

    subgraph "算法主循环 (每时间步)"
        F["开始新的时间步 (Global Step)"] --> G{"节点移动与环境变化"};
        G --> H{"全网状态测量 (fping, iw)"};
        H --> H1("测量 End-to-End 延迟, 丢包");
        H --> H2("测量 RSSI, 比特率, 多普勒频移");
        H --> I{"更新节点状态历史序列 (deque)"};
        I --> J{"核心节点动态选举"};
        J --> K["遍历所有核心节点 (独立 Actor)"];
    end

    subgraph "独立 Actor 的决策与学习"
        K --> L{"获取当前节点的状态序列"};
        L --> M["Actor 网络 (GRU-Policy)"];
        M --> M1("输入: 状态序列");
        M --> M2("GRU 处理: 捕捉时序趋势");
        M --> M3("输出: 动作概率分布, 价值估算");
        M --> N{"选择路由动作"};
        N --> O["执行路由动作 (下发主/备路径)"];
        O --> P{"计算即时奖励"};
        P --> Q["存储经验到节点的经验回放缓冲区"];
        Q --> R{"检查学习条件"};
        R -- "是" --> S["智能体学习 (agent.learn())"];
        S --> S1("从缓冲区采样批次经验");
        S --> S2("计算 Advantage");
        S --> S3("Actor Loss: PPO 裁剪目标");
        S --> S4("Critic Loss: MSE with Global Critic");
        S --> S5("反向传播与参数更新");
        S --> S6("更新 Actor & Shared Critic");
        R -- "否" --> K_end;
        S --> K_end;
        K_end["核心节点处理结束"];
    end

    subgraph "循环与评估"
        K_end --> T{"所有核心节点处理完毕?"};
        T -- "否" --> K;
        T -- "是" --> U["记录全局平均奖励"];
        U --> V{"达到最大训练步数?"};
        V -- "否" --> F;
        V -- "是" --> W["结束仿真与训练"];
        W --> X["性能评估与结果可视化"];
    end